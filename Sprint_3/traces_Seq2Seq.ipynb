{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from train_test_split import train_test_split_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "chosed_dataset = 'BPI_Challenge_2017_rfc_xgboost'\n",
    "df = pd.read_csv(f'../data/{chosed_dataset}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ######################################## TRAIN TEST INFO #######################################\n",
      "\n",
      "      Train set ends with 2016-10-27 09:12:04.286\n",
      "\n",
      "      Test set starts with: 2016-10-27 09:26:15.278\n",
      "\n",
      "    ################################################################################################\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split_custom(df=df, test_size=0.2, lags=True)\n",
    "\n",
    "# Convert timestamps to pandas datetime\n",
    "df['time:timestamp'] = pd.to_datetime(df['time:timestamp'])\n",
    "df.sort_values(by=['time:timestamp'], inplace=True)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "activity_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on all unique activities, including 'A_Create Application'\n",
    "all_activities = df['concept:name'].unique().tolist() + ['A_Create Application']\n",
    "activity_encoder.fit(all_activities)\n",
    "\n",
    "# Encode all activities in the dataset\n",
    "df['concept:name encoded'] = activity_encoder.transform(df['concept:name'])\n",
    "\n",
    "# Group by case and create sequences of activity codes\n",
    "sequences = df.groupby('case:concept:name')['concept:name encoded'].apply(list)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "max_seq_length = max(len(s) for s in sequences) + 1  # Plus one for the start token\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Create start tokens for each sequence\n",
    "start_activity_code = activity_encoder.transform(['A_Create Application'])[0]\n",
    "start_tokens = np.full((padded_sequences.shape[0], 1), start_activity_code)\n",
    "\n",
    "# Add start tokens to the beginning of each sequence\n",
    "padded_sequences = np.hstack((start_tokens, padded_sequences))\n",
    "\n",
    "# Prepare input (X) and target (Y) for the model\n",
    "X = padded_sequences[:, :-1]  # All but the last column\n",
    "Y = to_categorical(padded_sequences[:, 1:], num_classes=len(activity_encoder.classes_))  # One-hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 1s/step - accuracy: 0.8939 - loss: 0.4312 - val_accuracy: 0.9717 - val_loss: 0.0846\n",
      "Epoch 2/5\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 1s/step - accuracy: 0.9727 - loss: 0.0808 - val_accuracy: 0.9745 - val_loss: 0.0742\n",
      "Epoch 3/5\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 1s/step - accuracy: 0.9748 - loss: 0.0721 - val_accuracy: 0.9767 - val_loss: 0.0655\n",
      "Epoch 4/5\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 1s/step - accuracy: 0.9796 - loss: 0.0579 - val_accuracy: 0.9833 - val_loss: 0.0475\n",
      "Epoch 5/5\n",
      "\u001b[1m394/394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 1s/step - accuracy: 0.9839 - loss: 0.0454 - val_accuracy: 0.9849 - val_loss: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Define the Seq2Seq model architecture\n",
    "def build_seq2seq(input_dim, seq_len, embedding_dim=64, lstm_dim=256):\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=(seq_len,))\n",
    "    encoder_embedding = Embedding(input_dim=input_dim, output_dim=embedding_dim)(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = LSTM(lstm_dim, return_state=True)(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(seq_len,))\n",
    "    decoder_embedding = Embedding(input_dim=input_dim, output_dim=embedding_dim)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = TimeDistributed(Dense(input_dim, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Seq2Seq Model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate and compile the model\n",
    "seq_len = X.shape[1]\n",
    "input_dim = len(activity_encoder.classes_)\n",
    "seq2seq_model = build_seq2seq(input_dim, seq_len)\n",
    "seq2seq_model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = seq2seq_model.fit(\n",
    "    [X_train, X_train], Y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,  \n",
    "    validation_data=([X_test, X_test], Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 121ms/step\n",
      "Perplexity: 1.043305487657163\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predictions = seq2seq_model.predict([X_test, X_test])\n",
    "\n",
    "# Flatten the predictions and ground truth sequences\n",
    "flat_predictions = predictions.reshape(-1, predictions.shape[-1])\n",
    "flat_ground_truth = Y_test.reshape(-1, Y_test.shape[-1])\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "cross_entropy = -np.sum(flat_ground_truth * np.log(flat_predictions + 1e-10)) / len(flat_ground_truth)\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(cross_entropy)\n",
    "\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a sequence of activities\n",
    "def generate_full_sequence(model, input_seq, activity_encoder, max_length):\n",
    "    start_token = activity_encoder.transform(['A_Create Application'])[0]\n",
    "    decoder_input = np.zeros((1, max_length))\n",
    "    decoder_input[0, 0] = start_token  # setting the start token\n",
    "\n",
    "    output_seq = []\n",
    "\n",
    "    for i in range(1, max_length):\n",
    "        current_pred_probs = model.predict(\n",
    "            [input_seq, decoder_input], verbose=0)\n",
    "        current_pred = np.argmax(current_pred_probs[0, i - 1, :], axis=-1)\n",
    "        output_seq.append(current_pred)\n",
    "        decoder_input[0, i] = current_pred\n",
    "\n",
    "    decoded_sequence = activity_encoder.inverse_transform(output_seq)\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this cell runs for more than 1h but without any output. At first showing the ms/step but after 20min, VScode died. So I switch to this. need someone to run and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sequences for each case\n",
    "predicted_sequences = {}\n",
    "case_ids = sequences.index.tolist()\n",
    "\n",
    "for i, case_id in enumerate(case_ids):\n",
    "    input_seq = X[i:i+1]  # Select the encoder input for the current case\n",
    "    predicted_sequence = generate_full_sequence(seq2seq_model, input_seq, activity_encoder, max_seq_length)\n",
    "    predicted_sequences[case_id] = predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the predictions into a DataFrame\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "# Find the first known timestamp for each case to use as the initial timestamp for predictions\n",
    "last_timestamps = df.groupby('case:concept:name')['time:timestamp'].first()\n",
    "\n",
    "# Generate predictions and format them\n",
    "for case_id, predicted_seq in predicted_sequences.items():\n",
    "    initial_timestamp = last_timestamps[case_id]\n",
    "    # Generate timestamps for each predicted activity, spaced one hour apart\n",
    "    timestamps = [initial_timestamp + timedelta(hours=i) for i in range(len(predicted_seq))]\n",
    "    case_df = pd.DataFrame({\n",
    "        'case:concept:name': [case_id] * len(predicted_seq),\n",
    "        'concept:name': predicted_seq,\n",
    "        'time:timestamp': timestamps\n",
    "    })\n",
    "    predictions_df = predictions_df.append(case_df, ignore_index=True)\n",
    "\n",
    "# Save the formatted predictions to a CSV file\n",
    "predictions_csv_path = f'../data/Seq2Seq_predictions_{chosed_dataset}.csv'\n",
    "predictions_df.to_csv(predictions_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
