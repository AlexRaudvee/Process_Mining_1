{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '/Users/xuenichen/Desktop/Process_Mining_1-main/data'\n",
    "chosed_dataset = 'BPI_Challenge_2017'\n",
    "dataframe = pd.read_csv(f'{data_path}/train_test_{chosed_dataset}.csv')\n",
    "dataframe['time:timestamp'] = pd.to_datetime(dataframe['time:timestamp'])\n",
    "\n",
    "# Calculate average duration between events\n",
    "dataframe = dataframe.sort_values(['case:concept:name', 'time:timestamp'])\n",
    "\n",
    "dataframe['next_timestamp'] = dataframe.groupby('case:concept:name')['time:timestamp'].shift(-1)\n",
    "\n",
    "dataframe['duration'] = (dataframe['next_timestamp'] -dataframe['time:timestamp']).dt.total_seconds() / 60.0\n",
    "\n",
    "average_duration = dataframe['duration'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CSV file into an event log\n",
    "parameters = {\n",
    "    \"case_id_glue\": \"case:concept:name\",\n",
    "    \"activity_key\": \"concept:name\",\n",
    "    \"timestamp_key\": \"time:timestamp\"\n",
    "}\n",
    "event_log = log_converter.apply(\n",
    "    dataframe, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "# Encode activities\n",
    "activity_encoder = LabelEncoder()\n",
    "all_activities = [event['concept:name']for trace in event_log for event in trace]\n",
    "\n",
    "activity_encoder.fit(all_activities)\n",
    "\n",
    "# Encode all activities in the log\n",
    "for trace in event_log:\n",
    "    for event in trace:\n",
    "        event['concept:name'] = activity_encoder.transform([event['concept:name']])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq_model(input_vocab_size, output_vocab_size, latent_dim=256):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n",
    "    encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_embedding = Embedding(\n",
    "        output_vocab_size, latent_dim)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(\n",
    "        decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 770ms/step - accuracy: 0.9107 - loss: 0.3486 - val_accuracy: 0.9731 - val_loss: 0.0792\n",
      "Epoch 2/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 831ms/step - accuracy: 0.9732 - loss: 0.0772 - val_accuracy: 0.9741 - val_loss: 0.0726\n",
      "Epoch 3/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 775ms/step - accuracy: 0.9741 - loss: 0.0723 - val_accuracy: 0.9744 - val_loss: 0.0703\n",
      "Epoch 4/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 800ms/step - accuracy: 0.9742 - loss: 0.0709 - val_accuracy: 0.9760 - val_loss: 0.0658\n",
      "Epoch 5/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 786ms/step - accuracy: 0.9758 - loss: 0.0659 - val_accuracy: 0.9770 - val_loss: 0.0604\n",
      "Epoch 6/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 773ms/step - accuracy: 0.9774 - loss: 0.0591 - val_accuracy: 0.9782 - val_loss: 0.0564\n",
      "Epoch 7/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 774ms/step - accuracy: 0.9786 - loss: 0.0563 - val_accuracy: 0.9797 - val_loss: 0.0533\n",
      "Epoch 8/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 745ms/step - accuracy: 0.9794 - loss: 0.0542 - val_accuracy: 0.9798 - val_loss: 0.0530\n",
      "Epoch 9/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 788ms/step - accuracy: 0.9800 - loss: 0.0524 - val_accuracy: 0.9800 - val_loss: 0.0523\n",
      "Epoch 10/10\n",
      "\u001b[1m296/296\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 827ms/step - accuracy: 0.9803 - loss: 0.0518 - val_accuracy: 0.9810 - val_loss: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17f7e80d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input and target sequences from the encoded event log\n",
    "input_sequences = [[event['concept:name'] for event in trace[:-1]] for trace in event_log]\n",
    "\n",
    "target_sequences = [[event['concept:name'] for event in trace[1:]] for trace in event_log]\n",
    "\n",
    "# Pad sequences and prepare data for training\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "target_sequences = to_categorical(target_sequences, num_classes=len(activity_encoder.classes_))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "input_train, input_test, target_train, target_test = train_test_split(input_sequences, target_sequences, test_size=0.2)\n",
    "\n",
    "# Assuming build_seq2seq_model is defined and ready\n",
    "model = build_seq2seq_model(input_vocab_size=len(activity_encoder.classes_), output_vocab_size=len(activity_encoder.classes_))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit([input_train, input_train], target_train,batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions and Timestamps to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 214ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([input_test, input_test])\n",
    "predicted_sequences = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# Inverse transform to get the activity names back\n",
    "predicted_activities = [activity_encoder.inverse_transform(seq) for seq in predicted_sequences]\n",
    "\n",
    "# Using the minimum timestamp from the dataset as a starting point\n",
    "base_timestamp = datetime(2016, 1, 1)\n",
    "\n",
    "predicted_timestamps = []\n",
    "for activities in predicted_activities:\n",
    "    timestamps = []\n",
    "    current_timestamp = base_timestamp\n",
    "    for i in range(len(activities)):\n",
    "        timestamps.append(current_timestamp.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        # Increment the current timestamp by average_duration, with checks\n",
    "        next_timestamp = current_timestamp + \\\n",
    "            timedelta(minutes=average_duration)\n",
    "\n",
    "        # Safety check: Ensure the year is within a reasonable range\n",
    "        if next_timestamp.year < 2262:\n",
    "            current_timestamp = next_timestamp\n",
    "        else:\n",
    "            # If exceeding bounds, just replicate the last valid timestamp\n",
    "            # Max safe value for pandas\n",
    "            current_timestamp = datetime(2018, 1, 1)\n",
    "\n",
    "    predicted_timestamps.append(timestamps)\n",
    "    # Increment the base timestamp for the next trace\n",
    "    # Adjust this logic if necessary to ensure it's realistic for your dataset\n",
    "    base_timestamp = current_timestamp + \\\n",
    "        timedelta(minutes=average_duration * len(activities))\n",
    "\n",
    "# Pair each activity with its corresponding timestamp in a dictionary\n",
    "structured_traces = []\n",
    "for activities, timestamps in zip(predicted_activities, predicted_timestamps):\n",
    "    trace = [{'concept:name': act, 'time:timestamp': ts} for act, ts in zip(activities, timestamps)]\n",
    "    \n",
    "    structured_traces.append(json.dumps(trace))\n",
    "\n",
    "# Ensure `test_case_ids` matches the length of `predicted_activities`\n",
    "test_case_ids = [trace.attributes['concept:name'] for trace in event_log][:len(predicted_activities)]\n",
    "\n",
    "# Prepare and save the DataFrame\n",
    "df_predictions = pd.DataFrame({\n",
    "    'case:concept:name': test_case_ids,\n",
    "    'trace': structured_traces\n",
    "})\n",
    "\n",
    "# Specify your output path\n",
    "adjusted_output_path = os.path.join(data_path, f'Seq2Seq_predictions_{chosed_dataset}.csv')\n",
    "\n",
    "df_predictions.to_csv(adjusted_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
